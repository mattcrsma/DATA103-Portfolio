{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUOWsbJ6d9fY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data set\n",
        "url = \"healthcare-dataset-stroke-data.csv\"\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "cxRTCy2Rf5mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head())"
      ],
      "metadata": {
        "id": "KQSZGVAogBPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb151b56-1ec9-4b56-8354-b00d8af41226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
            "0   9046    Male  67.0             0              1          Yes   \n",
            "1  51676  Female  61.0             0              0          Yes   \n",
            "2  31112    Male  80.0             0              1          Yes   \n",
            "3  60182  Female  49.0             0              0          Yes   \n",
            "4   1665  Female  79.0             1              0          Yes   \n",
            "\n",
            "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
            "0        Private          Urban             228.69  36.6  formerly smoked   \n",
            "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
            "2        Private          Rural             105.92  32.5     never smoked   \n",
            "3        Private          Urban             171.23  34.4           smokes   \n",
            "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
            "\n",
            "   stroke  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.info())"
      ],
      "metadata": {
        "id": "RygV7GHRgDAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e466f4-00d7-4fb2-ca26-6dabbf68c675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 5110 non-null   int64  \n",
            " 1   gender             5110 non-null   object \n",
            " 2   age                5110 non-null   float64\n",
            " 3   hypertension       5110 non-null   int64  \n",
            " 4   heart_disease      5110 non-null   int64  \n",
            " 5   ever_married       5110 non-null   object \n",
            " 6   work_type          5110 non-null   object \n",
            " 7   Residence_type     5110 non-null   object \n",
            " 8   avg_glucose_level  5110 non-null   float64\n",
            " 9   bmi                4909 non-null   float64\n",
            " 10  smoking_status     5110 non-null   object \n",
            " 11  stroke             5110 non-null   int64  \n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 479.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for the null values of each column\n",
        "\n",
        "for each in data.columns:\n",
        "    print(f'There are {data[each].isnull().sum()} null values in the {each} column')"
      ],
      "metadata": {
        "id": "j1PzQB6igGmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9956e3-1b16-4761-f82a-e886811d7c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 null values in the id column\n",
            "There are 0 null values in the gender column\n",
            "There are 0 null values in the age column\n",
            "There are 0 null values in the hypertension column\n",
            "There are 0 null values in the heart_disease column\n",
            "There are 0 null values in the ever_married column\n",
            "There are 0 null values in the work_type column\n",
            "There are 0 null values in the Residence_type column\n",
            "There are 0 null values in the avg_glucose_level column\n",
            "There are 201 null values in the bmi column\n",
            "There are 0 null values in the smoking_status column\n",
            "There are 0 null values in the stroke column\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(inplace=True)\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "X = data.drop(columns=['stroke'])\n",
        "y = data['stroke']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RDk_TAWuKnXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading and cleaning data\n",
        "- we load the dataset and remove rows with missing values using dropna\n",
        "- we use pd.get_dummies to convert categorical columns into numerical representations for easier processing\n",
        "\n",
        "feature and target Selection\n",
        "- we define X as the features and y as the target (stroke)\n",
        "\n",
        "Data Splitting\n",
        "- using train_test_split, we create training and testing sets with 80-20 split"
      ],
      "metadata": {
        "id": "AWPNrTyiZMuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTreeCustom:\n",
        "    def __init__(self, maxDepth=3):\n",
        "        self.maxDepth = maxDepth\n",
        "        self.tree = []\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        if depth < self.maxDepth:\n",
        "            best_mse = float('inf')\n",
        "            n_samples, n_features = X.shape\n",
        "            for feature in range(n_features):\n",
        "                feature_vals = np.unique(X[:, feature])\n",
        "                for threshold in feature_vals:\n",
        "                    preds = np.where(X[:, feature] > threshold,\n",
        "                                     np.full(y.shape, y.mean()),\n",
        "                                     np.full(y.shape, y.mean() - y.std()))\n",
        "                    mse = np.mean((y - preds) ** 2)\n",
        "                    if mse < best_mse:\n",
        "                        best_mse = mse\n",
        "                        self.featureIndex = feature\n",
        "                        self.threshold = threshold\n",
        "            self.tree.append((self.featureIndex, self.threshold))\n",
        "\n",
        "    def predict(self, X):\n",
        "        feature_vals = X[:, self.featureIndex]\n",
        "        return np.where(feature_vals > self.threshold, 1, 0)"
      ],
      "metadata": {
        "id": "2ByGaMfvXVbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Stump\n",
        "- this custom decision tree serves as the \"weak learner\" in Gradient Boosting\n",
        "\n",
        "maxDepth\n",
        "- Limits the depth of the decision tree to prevent overfitting\n",
        "\n",
        "fit() method\n",
        "- it trains the decision tree using a simple splitting criterion, such as mean squared error (MSE) for regression or Gini impurity for classification (details may vary based on implementation)\n",
        "\n",
        "prediction\n",
        "- given input data, predict checks if each value is above the threshold and returns a binary prediction (1 or 0)"
      ],
      "metadata": {
        "id": "hJJWob0yY6Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBoostingClassifier:\n",
        "    def __init__(self, numEstimators=5, learningRate=0.1, maxDepth=3):\n",
        "        self.numEstimators = numEstimators\n",
        "        self.learningRate = learningRate\n",
        "        self.maxDepth = maxDepth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        preds = np.full(y.shape, y.mean())\n",
        "\n",
        "        for i in range(self.numEstimators):\n",
        "            residuals = y - preds\n",
        "            print(f\"Iteration {i + 1}\")\n",
        "            print(\"Residuals:\", residuals[:5])\n",
        "\n",
        "            tree = DecisionTreeCustom(maxDepth=self.maxDepth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            tree_preds = tree.predict(X)\n",
        "            preds += self.learningRate * tree_preds\n",
        "\n",
        "            print(\"Updated Predictions:\", preds[:5])\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learningRate * tree.predict(X)\n",
        "        return np.where(y_pred > 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "J_P43vx3YEo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n_estimators\n",
        "- the number of weak learners (decision trees) to use\n",
        "\n",
        "learning_rate\n",
        "- controls the contribution of each tree to the final prediction\n",
        "\n",
        "models\n",
        "- it just stores the trained weak learners\n",
        "\n",
        "fit method\n",
        "- the algorithm starts with the actual target values as the initial residuals\n",
        "- for each iteration, a weak learner (decision tree) is trained on the residuals\n",
        "- the predictions from this learner are scaled by the learning rate and subtracted from the residuals to update them\n",
        "- the trained weak learner is then appended to the list of models"
      ],
      "metadata": {
        "id": "d_tXkVCL8eZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model with fewer estimators for quick debugging\n",
        "model = GradientBoostingClassifier(numEstimators=10, learningRate=0.1, maxDepth=3)\n",
        "model.fit(X_train.values, y_train.values)"
      ],
      "metadata": {
        "id": "h2a6nTWRYgD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eed977d-4ad5-41c6-b454-e11134e2fbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Residuals: [-0.03972498 -0.03972498 -0.03972498 -0.03972498 -0.03972498]\n",
            "Updated Predictions: [0.13972498 0.13972498 0.13972498 0.13972498 0.13972498]\n",
            "----------------------------------------\n",
            "Iteration 2\n",
            "Residuals: [-0.13972498 -0.13972498 -0.13972498 -0.13972498 -0.13972498]\n",
            "Updated Predictions: [0.23972498 0.23972498 0.23972498 0.23972498 0.23972498]\n",
            "----------------------------------------\n",
            "Iteration 3\n",
            "Residuals: [-0.23972498 -0.23972498 -0.23972498 -0.23972498 -0.23972498]\n",
            "Updated Predictions: [0.33972498 0.33972498 0.33972498 0.33972498 0.33972498]\n",
            "----------------------------------------\n",
            "Iteration 4\n",
            "Residuals: [-0.33972498 -0.33972498 -0.33972498 -0.33972498 -0.33972498]\n",
            "Updated Predictions: [0.43972498 0.43972498 0.43972498 0.43972498 0.43972498]\n",
            "----------------------------------------\n",
            "Iteration 5\n",
            "Residuals: [-0.43972498 -0.43972498 -0.43972498 -0.43972498 -0.43972498]\n",
            "Updated Predictions: [0.53972498 0.53972498 0.53972498 0.53972498 0.53972498]\n",
            "----------------------------------------\n",
            "Iteration 6\n",
            "Residuals: [-0.53972498 -0.53972498 -0.53972498 -0.53972498 -0.53972498]\n",
            "Updated Predictions: [0.63972498 0.63972498 0.63972498 0.63972498 0.63972498]\n",
            "----------------------------------------\n",
            "Iteration 7\n",
            "Residuals: [-0.63972498 -0.63972498 -0.63972498 -0.63972498 -0.63972498]\n",
            "Updated Predictions: [0.73972498 0.73972498 0.73972498 0.73972498 0.73972498]\n",
            "----------------------------------------\n",
            "Iteration 8\n",
            "Residuals: [-0.73972498 -0.73972498 -0.73972498 -0.73972498 -0.73972498]\n",
            "Updated Predictions: [0.83972498 0.83972498 0.83972498 0.83972498 0.83972498]\n",
            "----------------------------------------\n",
            "Iteration 9\n",
            "Residuals: [-0.83972498 -0.83972498 -0.83972498 -0.83972498 -0.83972498]\n",
            "Updated Predictions: [0.93972498 0.93972498 0.93972498 0.93972498 0.93972498]\n",
            "----------------------------------------\n",
            "Iteration 10\n",
            "Residuals: [-0.93972498 -0.93972498 -0.93972498 -0.93972498 -0.93972498]\n",
            "Updated Predictions: [1.03972498 1.03972498 1.03972498 1.03972498 1.03972498]\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residuals\n",
        "- Each iteration outputs the first five values of the residuals to illustrate how much error remains after the current prediction.\n",
        "\n",
        "Updated Predictions\n",
        "- After updating with the stump’s prediction, it shows the first five updated predictions to help track learning progress over the boosting iterations.\n",
        "\n",
        "Iteration Information\n",
        "- The print statement at the beginning of each iteration clarifies which boosting round is currently running."
      ],
      "metadata": {
        "id": "3FsCQzjfYtdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DESCRIPTION"
      ],
      "metadata": {
        "id": "yt9o8TtIaVd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradient boosting is an ensemble technique that iteratively constructs a number of small models, frequently decision stumps, each of which fixes mistakes caused by the models before it. by incorporating new models that concentrate on residual errors, the objective is to reduce the discrepancy between expected and actual results.\n",
        "\n",
        "at each iteration the algorithm does as follows:\n",
        "\n",
        "- determines the residuals, or the discrepancies between the current projections and the actual target values.\n",
        "- fits these residuals to a weak learner (such a decision stump).\n",
        "- adds the new learner's predictions to the existing ones, scaling them according to the learning rate, a hyperparameter that regulates each learner's contribution.\n",
        "\n",
        "\n",
        "key concepts:\n",
        "\n",
        "weak learner\n",
        "- a simple model (e.g., a shallow decision tree) that performs slightly better than random guessing\n",
        "\n",
        "residuals\n",
        "- the difference between the actual values and the predicted values.\n",
        "\n",
        "learning rate\n",
        "- a scaling factor that controls how much each new model contributes to the overall prediction.\n",
        "\n",
        "additive training\n",
        "- models are trained sequentially, with each new model added to the ensemble to correct previous errors."
      ],
      "metadata": {
        "id": "5WoJHQ8RaZsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudocode for Gradient Boosting"
      ],
      "metadata": {
        "id": "KzZbN0-xbSlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Input: Training data (X, y), number of estimators (n_estimators), learning rate (learning_rate)\n",
        "Output: A trained Gradient Boosting model\n",
        "\n",
        "1. Initialize predictions as the mean of target values:\n",
        "   F_0(x) = mean(y)\n",
        "\n",
        "2. For each estimator (m = 1 to n_estimators):\n",
        "   a. Compute the residuals for each data point:\n",
        "      residual = y - F_(m-1)(x)\n",
        "\n",
        "   b. Train a weak learner (decision tree) on the residuals:\n",
        "      tree_m = TrainDecisionTree(X, residual)\n",
        "\n",
        "   c. Update predictions with the new weak learner's output scaled by the learning rate:\n",
        "      F_m(x) = F_(m-1)(x) + learning_rate * tree_m.predict(X)\n",
        "\n",
        "   d. Store the trained tree_m\n",
        "\n",
        "3. Final prediction for a new data point x':\n",
        "   F(x') = sum of (learning_rate * tree_m.predict(x')) for all m estimators\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "s1YJwo_HbXra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialization\n",
        "- to provide a baseline, begin with a straightforward prediction, such as the target variable's mean\n",
        "\n",
        "residual calculation\n",
        "- determine the difference between the current predictions and the actual target values for each iteration\n",
        "\n",
        "weak learner training\n",
        "- teach a basic model, or weak learner, to forecast these residuals\n",
        "\n",
        "update predictions\n",
        "- adding the new learner's predictions, weighted by a learning rate, will update the forecasts\n",
        "\n",
        "repeat\n",
        "- keep going until everything has been added"
      ],
      "metadata": {
        "id": "9epwXHPWbmrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differences Between Gradient Boosting and Random Forests"
      ],
      "metadata": {
        "id": "C1gipEMvb6bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. model combination\n",
        "\n",
        "- with gradient boosting, models are constructed one after the other, with each new model concentrating on the residual errors of the ones that came before it. we refer to this methodical process as \"boosting.\"\n",
        "- in contrast, random Forests construct each tree separately before averaging their predictions to produce a final outcome. We refer to this parallel strategy as \"bagging.\"\n",
        "\n",
        "2. prediction strategy\n",
        "\n",
        "- gradient Boosting generates predictions by adding together all of the weak learners' predictions, each of which is weighted by a learning rate.\n",
        "- random forests use the majority vote (for classification) or average (for regression) across all trees to generate predictions.\n",
        "\n",
        "3. complexity and interpretability\n",
        "\n",
        "- because each weak learner concentrates on fixing certain mistakes from the previous stage, gradient boosting frequently results in a more complex final model. although it needs to be carefully adjusted (e.g. the number of iterations and learning rate), it typically performs better on more difficult issues.\n",
        "- because Random Forests average independent trees, they are easier to understand and more resilient to overfitting, which allows them to be applied to a greater range of situations without requiring a lot of fine-tuning.\n",
        "\n",
        "Source:\n",
        "GeeksforGeeks. (2024, March 6). Gradient Boosting vs Random Forest. GeeksforGeeks. https://www.geeksforgeeks.org/gradient-boosting-vs-random-forest/\n",
        "\n",
        "‌"
      ],
      "metadata": {
        "id": "bzLqqGrOb9FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in short, random forests are parallel and concentrate on averaging results from independent models, whereas gradient boosting is sequential and modifies each model based on the errors of the previous one."
      ],
      "metadata": {
        "id": "LrwkCqiOciay"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CDh-8PMu-Vm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}